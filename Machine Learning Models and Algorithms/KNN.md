KNN
================
Haolin Zhong (UNI: hz2771)
2021/10/2

# KNN

## Core ideas

-   Classify one point by its K Nearest Neighborâ€™s category

-   Distances:

    -   Euclidean distance:
        ![d(x, y)=\\sqrt{\\displaystyle \\sum\_{k=1}^{n}\\left(x\_{k}-y\_{k}\\right)^{2}}](https://latex.codecogs.com/png.latex?d%28x%2C%20y%29%3D%5Csqrt%7B%5Cdisplaystyle%20%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%5Cleft%28x_%7Bk%7D-y_%7Bk%7D%5Cright%29%5E%7B2%7D%7D "d(x, y)=\sqrt{\displaystyle \sum_{k=1}^{n}\left(x_{k}-y_{k}\right)^{2}}")
    -   Manhattan:
        ![d(x, y)=\\sqrt{\\displaystyle \\sum\_{k=1}^{n}\\left\|x\_{k}-y\_{k}\\right\|}](https://latex.codecogs.com/png.latex?d%28x%2C%20y%29%3D%5Csqrt%7B%5Cdisplaystyle%20%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%5Cleft%7Cx_%7Bk%7D-y_%7Bk%7D%5Cright%7C%7D "d(x, y)=\sqrt{\displaystyle \sum_{k=1}^{n}\left|x_{k}-y_{k}\right|}")

## Practice

### Import dependency

``` python
import numpy as np
import pandas as pd

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
```

### Load and preprocess data

#### Inspect dataset

``` python
iris = load_iris() # It returns a dictionary containing feature, target and descriptions
iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names)

iris_df['class'] = iris.target
iris_df['class'] = iris_df['class'].map({0:iris.target_names[0], 1:iris.target_names[1], 2:iris.target_names[2]})

iris_df.describe()
```

    ##        sepal length (cm)        ...         petal width (cm)
    ## count         150.000000        ...               150.000000
    ## mean            5.843333        ...                 1.198667
    ## std             0.828066        ...                 0.763161
    ## min             4.300000        ...                 0.100000
    ## 25%             5.100000        ...                 0.300000
    ## 50%             5.800000        ...                 1.300000
    ## 75%             6.400000        ...                 1.800000
    ## max             7.900000        ...                 2.500000
    ## 
    ## [8 rows x 4 columns]

#### Define features and target

``` python
X = iris.data
Y = iris.target.reshape(-1, 1)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, 
                                                    random_state = 35, 
                                                    stratify = Y)
# stratify = Y: keep proportion of Y same with original data in train/test data 
```

### Core algorithms

``` python
# distance function

def l1_distance(a, b):
    # a can be a matrix, be must be a vector
    # axis = 1: sum at row level and return one column
    return np.sum(np.abs(a-b), axis = 1)


def l2_distance(a, b):
    return np.sqrt(np.sum((a-b)**2, axis = 1))
  
  
# classfication

class KNN(object):
    def __init__(self, n_neighbors = 1, dist_func = l1_distance):
        self.n_neighbors = n_neighbors
        self.dist_func = dist_func
    
    def fit(self, X, Y):
        # for KNN, there is actually no fitting process. We only need marked train data.
        self.X_train = X
        self.Y_train = Y
        
    def predict(self, X_test):
        Y_pred = np.zeros((X_test.shape[0], 1), dtype = self.Y_train.dtype)
        
        for i, x_test in enumerate(X_test):
            # calculate the distance between this test points and every train points
            distances = self.dist_func(self.X_train, x_test)
            
            # order by distance, get index value
            nn_index = np.argsort(distances)
            
            # select nearest k points, store their categories
            nn_Y = self.Y_train[nn_index[:self.n_neighbors]].ravel()
                # ravel: reshape into 1D array
            
            # find the category which appears most
            Y_pred[i] = np.argmax(np.bincount(nn_Y))
                # bincount: count how many times that each value apppears
        
        return Y_pred
```

### Test

``` python
knn = KNN(n_neighbors = 3)
knn.fit(X_train, Y_train)
Y_pred = knn.predict(X_test)

# Calculate accuracy

accuracy = accuracy_score(Y_test, Y_pred)

print("Prediction accuracy:" + str(accuracy))
```

    ## Prediction accuracy:0.9333333333333333

#### parameter selection

``` python
knn = KNN()
knn.fit(X_train, Y_train)
results = []

# parameter selection
# select distance function and n_neighbors

for p in [1, 2]:
    knn.dist_func = l1_distance if p == 1 else l2_distance
    
    for k in range(1, 10, 2):
        knn.n_neighbors = k
        Y_pred = knn.predict(X_test)
        accuracy = accuracy_score(Y_test, Y_pred)
        results.append([k, p, accuracy])


df = pd.DataFrame(results, columns = ['k', 'dist_func', 'accuracy'])

df
```

    ##    k  dist_func  accuracy
    ## 0  1          1  0.933333
    ## 1  3          1  0.933333
    ## 2  5          1  0.977778
    ## 3  7          1  0.955556
    ## 4  9          1  0.955556
    ## 5  1          2  0.933333
    ## 6  3          2  0.933333
    ## 7  5          2  0.977778
    ## 8  7          2  0.977778
    ## 9  9          2  0.977778
